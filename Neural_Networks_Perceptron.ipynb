{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='turquoise'>**NEURAL NETWORKS**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural networks** are a category of machine learning models inspired by the human brain. They are designed to recognize patterns and interpret sensory data through a kind of machine perception, labeling or clustering raw input.\n",
    "\n",
    "A neural network takes in inputs, which are then processed in hidden layers using weights that are adjusted during training. Then the model spits out a prediction. The weights are adjusted to find patterns in order to make better predictions.\n",
    "\n",
    "The network consists of layers. Each layer has multiple neurons (also called nodes). The layers are categorized into three types:\n",
    "\n",
    "1. **Input Layer:** This layer receives all the inputs and forwards them to the hidden layer for analysis. Each node in the input layer represents one feature.\n",
    "\n",
    "2. **Hidden Layer(s):** These are layers between the input and output layers. The magic of neural networks happens here, through the weights and activation functions. There can be multiple hidden layers in a network.\n",
    "\n",
    "3. **Output Layer:** This layer receives inputs from the last hidden layer and transforms them into the format suitable for the problem at hand (e.g., for a binary classification problem, it would output a probability score near 0 or 1).\n",
    "\n",
    "The power of neural networks comes from their ability to learn complex patterns and relationships from data, making them highly valuable for tasks like image and speech recognition, natural language processing, and other complex tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='sky blue'>**Activation Functions**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions are used in neural networks to introduce non-linearity into the network, allowing it to learn from complex data. Here are some common activation functions:\n",
    "\n",
    "1. **Sigmoid Function:** The Sigmoid function is a smooth, S-shaped function that maps any real-valued number to a value between 0 and 1. It's often used in the output layer of a binary classification network.\n",
    "\n",
    "    ```python\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    ```\n",
    "\n",
    "2. **Tanh (Hyperbolic Tangent) Function:** The tanh function is similar to the sigmoid function but maps any real-valued number to a value between -1 and 1. It's often used in hidden layers of a neural network.\n",
    "\n",
    "    ```python\n",
    "    def tanh(x):\n",
    "        return np.tanh(x)\n",
    "    ```\n",
    "\n",
    "3. **ReLU (Rectified Linear Unit) Function:** The ReLU function outputs the input directly if it's positive; otherwise, it outputs zero. It's often used in hidden layers of a neural network.\n",
    "\n",
    "    ```python\n",
    "    def relu(x):\n",
    "        return np.maximum(0, x)\n",
    "    ```\n",
    "\n",
    "4. **Leaky ReLU Function:** The Leaky ReLU function is a variant of ReLU that allows small negative values when the input is less than zero. It's used to fix the \"dying ReLU\" problem where a neuron might always output a negative value and therefore cannot make any progress since the gradient of ReLU is zero for negative inputs.\n",
    "\n",
    "    ```python\n",
    "    def leaky_relu(x):\n",
    "        return np.maximum(0.01 * x, x)\n",
    "    ```\n",
    "\n",
    "5. **Softmax Function:** The Softmax function is often used in the output layer of a neural network for multi-class classification problems. It converts a vector of numbers into a vector of probabilities, where the probabilities of each value are proportional to the relative scale of each value in the vector.\n",
    "\n",
    "    ```python\n",
    "    def softmax(x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "    ```\n",
    "\n",
    "Each of these activation functions has its own use cases and characteristics, and the choice of activation function can significantly impact the performance of a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='sky blue'>**Perceptron: a signle neuron with one layer**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single neuron with one layer is the simplest form of a neural network.\n",
    "\n",
    "1. **Input:** The neuron receives one or more inputs. Each input comes with an associated weight which can be adjusted during the learning process. The weight increases the importance of the input if it's positive, or diminishes it if it's negative.\n",
    "\n",
    "2. **Weighted Sum:** The neuron calculates the weighted sum of the inputs. This is done by multiplying each input by its corresponding weight and then adding them together.\n",
    "\n",
    "3. **Add Bias:** The neuron then adds a bias term. The bias allows the neuron to shift the activation function to the left or right, which can be critical for successful learning.\n",
    "\n",
    "4. **Activation Function:** The result of the weighted sum plus the bias is then passed through an activation function. The activation function introduces non-linearity into the output of a neuron. This non-linearity allows neural networks to learn from error and make adjustments, and it's what allows the network to model complex patterns.\n",
    "\n",
    "5. **Output:** The result of the activation function is the final output of the neuron.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "#The training data, X are features and Y are the labels \n",
    "X = np.array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
    "Y = np.array([[0, 0, 1, 1]]).T\n",
    "print(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, deriv=False):\n",
    "    if deriv == True:\n",
    "        return x * (1 - x)\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.49737296]\n",
      " [ 0.55432718]\n",
      " [-0.65129331]]\n"
     ]
    }
   ],
   "source": [
    "#Initializing the weights to random small values\n",
    "W = 2 * np.random.random((3, 1)) - 1\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after training\n",
      "[[13.11370854]\n",
      " [-0.20370782]\n",
      " [-6.35334792]]\n",
      "Output after training\n",
      "[[0.00173789]\n",
      " [0.00141805]\n",
      " [0.99884253]\n",
      " [0.99858138]]\n"
     ]
    }
   ],
   "source": [
    "#The alogrithm for training the simple neural network\n",
    "i = 0\n",
    "while i < 100000: #number of iterations\n",
    "    i += 1\n",
    "    output = sigmoid(np.dot(X, W)) #The output from the neural network\n",
    "    error = Y - output #The error by comparing the output with the expected output Y in our case\n",
    "    '''The reason for multiplying the error by the derivative of the sigmoid function is to implement the gradient descent algorithm, \n",
    "        which is used to optimize the weights of the network.\n",
    "      The derivative indicates the direction and rate of change of the error with respect to the weights, \n",
    "      so it's used to adjust the weights in the direction that most decreases the error.'''\n",
    "    delta = error * sigmoid(output, True) #The delta value for updating the weights\n",
    "    W = W + np.dot(X.T, delta)  #Updating the weights\n",
    "\n",
    "\n",
    "print(\"Weights after training:\")\n",
    "print(W)\n",
    "\n",
    "print(\"Output after training:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for [1, 0, 0]:\n",
      "[0.99999798]\n"
     ]
    }
   ],
   "source": [
    "#Predicting the class of this observation [1, 0, 0] using the neural network after training (with the updated weights)\n",
    "print(\"Prediction for [1, 0, 0]:\")\n",
    "print(sigmoid(np.dot(np.array([1, 0, 0]), W)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
